# Identifying High-level Structures Through Isolated Updates

This repo contains code for training an RL agent in an environment to optimize a reward, and then training it for a few timesteps on a changed reward.  This is to test one of my theories of interpretability that if you can change one aspect of the training setup after training the model to a sufficient limit, whatever update there are to the weights will correspond to that change, and that this can be used to isolate a pure high-level structure of some property in the network.

For example, I train a PPO agent in the LunarLander-v2 enviroment to optimize landing the lander on a landing pad without crashing.  Then, I train it for a few timesteps on a changed reward that penalizes fuel consumption in the lander. Assuming the theory to be correct, the updates to the weights in these timesteps should correspond to the model's reward function (or some portion thereof).

Of course, there is further work involved in trying to isolate this high-level structure purely.  One approach considered is to subtract these updates from the model prior to this second stage of training and observing whether the model now performs as well on the landing task, while *maximizing* fuel consumption - a priori this may well have no reason to be true if reward is represented by pointers instead of intrinsically, or if there are additional policy changes whose inverse mechanistically do not imply the inverse of the reward function. Further, there could be randomness in the updates during the second phase that could lead to subtracting them hampering the model in other ways.  One potential fix to this is to train the model for a few timesteps more, to see if it regains that capability, and how long it takes to do so.